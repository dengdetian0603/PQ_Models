plot(fit0.3, mark.time=FALSE,lty = 1,col=4,xmax=1500/30, conf.int=FALSE, axes=FALSE)
legend(0,1.65, col=c(1:4,6), lty=1, legend=c("K-M estimate with censoring",
"Random imputation","1st predictive iteration",
"2nd predictive iteration", "Probability Density of Censoring"))
mtext(side = 1, line = 3, "Time to Event (month)")
par(xpd=FALSE)
downsample = sample(1:1600, 600)
D = as.numeric(Y.train$DEATH[downsample])
load("~/Documents/JHSPH/Research/DreamChallenge/report/cvPredict03022016.Rda")
layout(matrix(c(1,2,3,3), ncol=2, byrow=TRUE), heights=c(4, 1))
par(mai=c(1,1,0.2,0.2))
plot(Y.train$LKADT_P[downsample]/30, avg.OutOfSamplePrediction[downsample]/30, xlab="Observed Time (month)", col= D,
ylab="Out-of-bag Predicted Time", pch= (D-1)*18, xlim=c(0,50), ylim=c(0,50) , xaxp  = c(0, 48, 4))
abline(a=0,b=1,col="red")
D
plot(Y.train$LKADT_P[downsample]/30, avg.OutOfSamplePrediction[downsample]/30, xlab="Observed Time (month)", col= D,
ylab="Out-of-bag Predicted Time", pch= (D-1)*18, xlim=c(0,50), ylim=c(0,50) , xaxp  = c(0, 48, 4))
abline(a=0,b=1,col="red")
plot(Y.train$LKADT_P[downsample]/30, avg.OutOfSamplePrediction[downsample]/30, xlab="Observed Time (month)", col= D,
ylab="Out-of-bag Predicted Time", pch= (D-1)*18, xlim=c(0,50), ylim=c(0,50) , xaxp  = c(0, 48, 4))
abline(a=0,b=1,col="red")
plot(Y.train$LKADT_P[downsample]/30, avg.OutOfSamplePrediction[downsample]/30, xlab="Observed Time (month)", col= D,
ylab="Out-of-bag Predicted Time", pch= (D-1)*18, xlim=c(0,50), ylim=c(0,50) , xaxp  = c(0, 48, 4))
abline(a=0,b=1,col="red")
plot(Y.train$LKADT_P[downsample]/30, t.imp.avg[downsample]/30, xlab="Observed Time (month)", col=D,
ylab="Imputed Time", pch=(D-1)*18, xlim=c(0,50), ylim=c(0,50),  xaxp  = c(0, 48, 4))
abline(a=0,b=1,col="red")
layout(matrix(c(1,2,3,3), ncol=2, byrow=TRUE), heights=c(4, 1))
par(mai=c(1,1,0.2,0.2))
plot(Y.train$LKADT_P[downsample]/30, avg.OutOfSamplePrediction[downsample]/30, xlab="Observed Time (month)", col= D,
ylab="Out-of-bag Predicted Time", pch= (D-1)*18, xlim=c(0,50), ylim=c(0,50) , xaxp  = c(0, 48, 4))
abline(a=0,b=1,col="red")
plot(Y.train$LKADT_P[downsample]/30, t.imp.avg[downsample]/30, xlab="Observed Time (month)", col=D,
ylab="Imputed Time", pch=(D-1)*18, xlim=c(0,50), ylim=c(0,50),  xaxp  = c(0, 48, 4))
abline(a=0,b=1,col="red")
par(mai=c(0,0,0,0))
plot.new()
legend("center", col=1:2, pch=c(0,18), legend=c("Censored Observations", "Actual Survival Times"))
layout(matrix(1))
downsample = sample(1:1600, 600)
D = as.numeric(Y.train$DEATH[downsample])
layout(matrix(c(1,2,3,3), ncol=2, byrow=TRUE), heights=c(4, 1))
par(mai=c(1,1,0.2,0.2))
plot(Y.train$LKADT_P[downsample]/30, avg.OutOfSamplePrediction[downsample]/30, xlab="Observed Time (month)", col= D,
ylab="Out-of-bag Predicted Time", pch= (D-1)*18, xlim=c(0,50), ylim=c(0,50) , xaxp  = c(0, 48, 4))
abline(a=0,b=1,col="red")
plot(Y.train$LKADT_P[downsample]/30, t.imp.avg[downsample]/30, xlab="Observed Time (month)", col=D,
ylab="Imputed Time", pch=(D-1)*18, xlim=c(0,50), ylim=c(0,50),  xaxp  = c(0, 48, 4))
abline(a=0,b=1,col="red")
par(mai=c(0,0,0,0))
plot.new()
legend("center", col=1:2, pch=c(0,18), legend=c("Censored Observations", "Actual Survival Times"))
layout(matrix(1))
load("~/Documents/JHSPH/Research/DreamChallenge/report/today.Rda")
library(survival)
t.imp.avg = exp(apply(Y.imp.new2, 1, mean))
death.imp = rep(1, 1600)
dat.imp = data.frame(Time1 = exp(apply(Y.imp, 1, mean)),
Time2 = exp(apply(Y.imp.new, 1, mean)),
Time3 = t.imp.avg,
Death = death.imp, study = Y.train$study)
Y.train$DEATH = (Y.train$DEATH==1)
fit0 = survfit(Surv(LKADT_P/30, DEATH) ~ 1, data = Y.train, conf.type="log-log")
#fit1 = survfit(Surv(LKADT_P, DEATH) ~ study, data = Y.train)
#plot(fit0, mark.time=FALSE, lty = 1,col=1:3,xmax=42)
#plot(fit1, mark.time=FALSE,lty = 1,col=1:3,xmax=1200, conf.int=TRUE)
fit0.1 = survfit(Surv(Time1/30, Death) ~ 1, data = dat.imp)
fit0.2 = survfit(Surv(Time2/30, Death) ~ 1, data = dat.imp)
fit0.3 = survfit(Surv(Time3/30, Death) ~ 1, data = dat.imp)
C = Y.train$LKADT_P[Y.train$DEATH==0]/30
par(mar=c(6, 4, 9, 5), xpd=TRUE)
plot(fit0, mark.time=FALSE, lty = 1,col=1,xmax=1500/30, conf.int=FALSE,  xaxp  = c(0, 48, 4))
par(new=TRUE)
plot(density(C, from=-50/30, to=1500/30),ylim=c(0,0.2),xlim=c(0,1500/30), axes=FALSE, xlab="", ylab="Survival Probability", main="",col="6")
axis(side = 4)
mtext(side = 4, line = 3, "Density of Censoring")
par(new=TRUE)
plot(fit0.1, mark.time=FALSE,lty = 1,col=2,xmax=1500/30, conf.int=FALSE, axes=FALSE)
par(new=TRUE)
plot(fit0.2, mark.time=FALSE,lty = 1,col=3,xmax=1500/30, conf.int=FALSE, axes=FALSE)
par(new=TRUE, xpd=TRUE)
plot(fit0.3, mark.time=FALSE,lty = 1,col=4,xmax=1500/30, conf.int=FALSE, axes=FALSE)
legend(0,1.65, col=c(1:4,6), lty=1, legend=c("K-M estimate with censoring",
"Random imputation","1st predictive iteration",
"2nd predictive iteration", "Probability Density of Censoring"))
mtext(side = 1, line = 3, "Time to Event (month)")
par(xpd=FALSE)
#par(new=TRUE)
#plot(predcurv.med, mark.time=FALSE,lty = 1,col=6,xmax=1500, conf.int=FALSE)
#par(new=FALSE)
#--------------------------------
# fit1.1 = survfit(Surv(Time, Death) ~ study, data = dat.imp)
# plot(fit1.1, mark.time=FALSE,lty = 1,col=1:3,xmax=1200, conf.int=TRUE)
#
# #-----------------------------------
# Dat.cox1 = cbind(Y.train, Dat.X_clean_imputed_1[1:1600,-1])
# fml = as.formula(paste("Surv(LKADT_P, DEATH) ~", paste(names(Dat.cox1)[c(6:8,10,12:25,27:29,31,33:52,54:89)], collapse= "+")))
# fit2.0 = coxph(fml , data = Dat.cox1); summary(fit2.0)
# basehaz(fit2.0, TRUE)
# plot(survfit(fit2.0, newdata = Dat.cox1[1:20,]), mark.time=FALSE)
# par(new=TRUE)
# predcurvs = survfit(fit2.0, newdata = Dat.cox1[sample(1:1600,300),])
# predcurvs = survfit(fit2.0, newdata = Dat.cox1[1:1600,])
# predcurv.med = predcurvs
# predcurv.med$surv = matrix(apply(predcurvs$surv, 1, median), ncol=1)
# plot(predcurvs,  mark.time=FALSE ,col=7, xmax=1500, conf.int=FALSE)
#plot(predcurv.med,  mark.time=FALSE, col=7, xmax=1500, conf.int=FALSE)
# #-----------------------------------
# getQsurv = function(S, T){
#       q25 = sum(S>=0.25, na.rm=TRUE)
#       q40 = sum(S>=0.40, na.rm=TRUE)
#       q50 = sum(S>=0.50, na.rm=TRUE)
#       q60 = sum(S>=0.60, na.rm=TRUE)
#       q75 = sum(S>=0.75, na.rm=TRUE)
#       T[c(q25, q40, q50, q60, q75)]
# }
# getQsurv(predcurvs$surv[,1598], predcurvs$time)
# Qsurv = apply(predcurvs$surv[,1:1600], 2, getQsurv, T=predcurvs$time)
#
# Qsurv.data = cbind(Y.train$LKADT_P, Y.train$DEATH, t(Qsurv))
# plot(Qsurv.data[downsample,1], Qsurv.data[downsample,3], xlab="observed time",
#      ylab="predicted survival quantile", pch=Qsurv.data[downsample,2]*18, xlim=c(0,1500), ylim=c(0,1500))
# abline(a=0,b=1,col="red")
#
# plot(Qsurv.data[downsample,1], Qsurv.data[downsample,5], xlab="observed time",
#      ylab="predicted survival quantile", pch=Qsurv.data[downsample,2]*18,
#      xlim=c(0,1500), ylim=c(0,1500),asp=1)
# abline(a=0,b=1,col="red")
# abline(v=0,col="black")
# abline(h=0,col="black")
load("~/Documents/JHSPH/Research/DreamChallenge/report/cvPredict03022016.Rda")
downsample = sample(1:1600, 600)
D = as.numeric(Y.train$DEATH[downsample])
layout(matrix(c(1,2,3,3), ncol=2, byrow=TRUE), heights=c(4, 1))
par(mai=c(1,1,0.2,0.2))
plot(Y.train$LKADT_P[downsample]/30, avg.OutOfSamplePrediction[downsample]/30, xlab="Observed Time (month)", col= D,
ylab="Out-of-bag Predicted Time", pch= (D-1)*18, xlim=c(0,50), ylim=c(0,50) , xaxp  = c(0, 48, 4))
abline(a=0,b=1,col="red")
plot(Y.train$LKADT_P[downsample]/30, t.imp.avg[downsample]/30, xlab="Observed Time (month)", col=D,
ylab="Imputed Time", pch=(D-1)*18, xlim=c(0,50), ylim=c(0,50),  xaxp  = c(0, 48, 4))
abline(a=0,b=1,col="red")
par(mai=c(0,0,0,0))
plot.new()
legend("center", col=1:2, pch=c(0,18), legend=c("Censored Observations", "Actual Survival Times"))
layout(matrix(1))
set.seed(649)
data = rnorm(200,0,5)
save.image("~/Desktop/TA646/hw1data.RData")
inv.logit = function(x)
{
1/(1+exp(-x))
}
par0 = c(-0.8472979, 0.6190392, -1.386294, 0.4054651, -1.386294)
inv.logit(par0)
round(inv.logit(par0),4)
bias.ex1.truth =  c(-0.01581599, -0.002502574, -0.01598298, -0.00421396, 0.01085477)
bias.ex0.truth = c(-0.01662488, 0.01262615, -0.04371849, -0.003699009, 0.001240989)
bias.ex1.prior = c( -0.5532704, 1.094511, -0.1468577, 1.521978, 0.09652053)
bias.ex0.prior = c(-0.5614301, 1.170443, -0.1303212, 1.517447, 0.1164624)
round(inv.logit(par0),4)
beta1 = par0 + bias.ex1.truth
beta2 = par0 + bias.ex0.truth
beta3 = par0 + bias.ex1.prior
beta4 = par0 + bias.ex0.prior
mu0 = round(inv.logit(par0),4)
mu1 = round(inv.logit(beta1),4)
mu2 = round(inv.logit(beta2),4)
mu3 = round(inv.logit(beta3),4)
mu4 = round(inv.logit(beta4),4)
cbind(mu0, mu1, mu2, mu3, mu4)
curve(dbeta)
curve(dbeta, 0,1)
plot(density(rbeta(1000,1,9)))
lines(density(rbeta(1000,1,9)),col=2)
plot(density(rbeta(10000,1,9)))
lines(density(rbeta(1000,1,6)),col=2)
lines(density(rbeta(1000,1,19)),col=3)
lines(density(rbeta(1000,10,90)),col=2)
plot(density(rbeta(10000,2,18)))
lines(density(rbeta(1000,3,27)),col=2)
plot(density(rbeta(10000,3,27)))
lines(density(rbeta(1000,3,18)),col=2)
lines(density(rbeta(1000,3,57)),col=3)
devtools::install_github("muschellij2/swirl", ref = "dev")
library(devtools)
install.packages('devtools')
devtools::install_github("muschellij2/swirl", ref = "dev")
library(swirl)
install_from_swirl("Getting_and_Cleaning_Data")
swirl()
mydf  <- read.csv(path2csv, stringsAsFactors = FALSE)
dim(mydf)
head
(mydf)
head(mydf)
library(dplyr)
packageVersion('dplyr')
cran  <- tbl_df(mydf)
rm('mydf')
print(tbl_df)
print(cran)
cran
?select
select(cran, ip_id, package, country)
5:20
select(cran, r_arch:country)
select(cran, country:r_arch)
cran
select(cran, -time)
-5:20
-(5:20)
select(cran, -(X:size))
filter(cran, package == "swirl")
filter(cran, r_version == "3.1.1", country == "US")
?Comparison
filter(cran, r_version < "3.0.2", country == "IN")
filter(cran, r_version <= "3.0.2", country == "IN")
filter(cran, country == "US" | country == "IN")
filter(cran, size > 100500)
filter(cran, size > 100500, r_os == "linux-gnu")
is.na(c(3, 5, NA, 10))
!is.na(c(3, 5, NA, 10))
filter(cran, size > 100500, r_os == "linux-gnu")
filter(cran, !is.na(r_version))
cran2  <- select(cran, size:ip_id)
arrange(cran2, ip_id)
arrange(cran2, desc(ip_id))
arrange(cran2, package, ip_id)
arrange(cran2, country, desc(r_version), ip_id)
cran3 <- select(cran, ip_id, package, size)
cran3
mutate(cran3, size_mb = size / 2^20)
mutate(cran3, size_gb = size_mb / 2^10)
mutate(cran3, size_mb = size / 2^20, size_gb = size_mb / 2^10)
mutate(cran3, correct_size = size + 1000)
summarize(cran, avg_bytes = mean(size))
library(dplyr)
cran  <- tbl_df(mydf)
rm('mydf')
cran
?group_by
by_package  <- group_by(cran, package)
by_package
summarize(by_package, mean(size))
submit()
submit()
pack_sum
quantile(pack_sum$count, probs = 0.99)
top_counts <- filter(by_package, count > 679)
top_counts <- filter(pack_sum, count > 679)
top_counts
View(top_counts)
top_counts_sorted <- arrange(top_counts, desc(count))
View(top_counts_sorted)
quantile(pack_sum$unique, probs = 0.99)
top_unique <- filter(pack_sum, unique > 465)
View(top_unique)
arrange(top_unique, desc(unique))
arrange(top_unique, desc(unique))->top_unique_sorted
View(top_unique_sorted)
submit()
submit()
submit()
View(result3)
submit()
submit()
submit()
submmit()
submit()
library(tidyr)
students
?gather
gather(student, student, sex, count, -grade)
gather(students, student, sex, count, -grade)
gather(students, sex, count, -grade)
students2
gather(students2, sex_class, count)->res
gather(students2, sex_class, count, -grade)->res
res
?separate
separate(res, sex_class, c("sex","class"), sep="_")
separate(res, sex_class, c("sex","class"))
submit()
students3
?gather
submit()
?spread
submit()
submit()
submit()
submit()
reset()
swirl()
submit()
submit()
submit()
library(readr)
parse_numeric("class5")
submit()
students4
submit()
submit()
submit()
passed
failed
passed <- mutate(passed, status = 'passed')
faileded <- mutate(failed, status = 'failed')
failed <- failed %>% mutate(status = "failed")
?bind_rows
bind_rows(passed, failed)
sat
?separate
submit()
submit()
submit()
x = rbeta(500, 2, 3)
y = logit(x)
x = rbeta(500, 2, 3)
y = log(x/(1-x))
qqplot(y)
qqnorm(y)
abline(a = 0, b = 1)
x = rbeta(500, 2, 3)
y = log(x/(1-x)) - log(4/6)
plot(density(y))
qqnorm(y)
abline(a = 0, b = 1)
x = rbeta(5000, 2, 3)
y = log(x/(1-x)) - log(4/6)
plot(density(y))
qqnorm(y)
abline(a = 0, b = 1)
install.packages("spcov")
install.packages("robust")
install.packages("infotheo")
library(infotheo)
dat = data.frame(x1 = rbeta(300, 2,1), x2 = rbeta(300, 20, 10), x3 = rbeta(300, 1,1,))
interinformation(dat)
interinformation(dat, "mm")
interinformation(dat, "shrink")
interinformation(dat, "sg")
multiinformation(dat, "sg")
multiinformation(dat, "mm")
dat2 = discretize(dat)
dat2
multiinformation(dat2)
interinformation(dat2)
mutinformation(dat2$x1, dat$x2)
mutinformation(dat2$x1, dat2$x2)
dat = data.frame(x1 = rbeta(3000, 2,1), x2 = rbeta(3000, 20, 10), x3 = rbeta(3000, 1,1,))
dat = discretize(dat)
mutinformation(dat$x1, dat$x2)
dat = data.frame(x1 = rbeta(3000, 2,1), x2 = rbeta(3000, 20, 10), x3 = rbeta(3000, 2,1,))
dat = discretize(dat)
mutinformation(dat$x1, dat$x2)
mutinformation(dat$x1, dat$x3)
dat = data.frame(x1 = rbeta(3000, 2,1), x2 = rbeta(3000, 20, 10), x3 = rbeta(3000, 2,1))
dat = discretize(dat)
dat
plot(dat$x1, dat$x3)
hist(dat$x1)
dist(dat$x3)
hist(dat$x2)
hist(dat$x1)
))
hist(dat$x2)
dat = data.frame(x1 = rbeta(3000, 2,1), x2 = rbeta(3000, 20, 10), x3 = rbeta(3000, 2,1))
hist(discretize(dat$x1, "equalwidth", 30))
discretize(dat$x1, "equalwidth", 30)
hist(discretize(dat$x1, "equalwidth", 30)[,1])
hist(discretize(dat$x1, "equalfref", 30)[,1])
hist(discretize(dat$x1, "equalfreq", 30)[,1])
hist(discretize(dat$x1, "equalwidth", 30)[,1])
hist(discretize(dat$x1, "equalwidth", 60)[,1])
hist(discretize(dat$x2, "equalwidth", 60)[,1])
hist(discretize(dat$x3, "equalwidth", 60)[,1])
dat = discretize(dat, "equalwidth", 40)
mutinformation(dat$x1, dat$x3)
mutinformation(dat$x1, dat$x2)
mutinformation(dat$x3, dat$x2)
mutinformation(dat$x2, dat$x1)
mutinformation(dat$x2, dat$x3)
mutinformation(dat$x2, dat$x3)/entropy(dat$x3)
mutinformation(dat$x1, dat$x3)/entropy(dat$x3)
mutinformation(dat$x3, dat$x3)/entropy(dat$x3)
mutinformation(dat$x1, dat$x3)/entropy(dat$x3)
dat = data.frame(x1 = rbeta(3000, 4,2), x2 = rbeta(3000, 20, 10), x3 = rbeta(3000, 4,2))
dat = discretize(dat, "equalwidth", 50)
mutinformation(dat$x3, dat$x3)/entropy(dat$x3)
mutinformation(dat$x1, dat$x3)/entropy(dat$x3)
mutinformation(dat$x2, dat$x3)/entropy(dat$x3)
entropy(dat$x3)
install.packages(mpmi)
install.packages("mpmi")
library(mpmi)
install.packages("mpmi")
library(mpmi)
?install.packages
setwd("~/Documents/JHSPH/Research/S.Zeger/PQ_Models/Code")
source("./RawDataProcess.R")
source("./PerchDataSimulation.R")
source("./ModelFittingJAGS.R")
source("./PostProcessJAGS.R")
source("./PerchPlots.R")
source("../../BAKER/utils.R")
library(ggplot2)
load("~/Documents/JHSPH/Research/S.Zeger/PQ_Models/WorkSpace/PQ_20170309/SAF_HIV_analysis.Rdata")
summary(as.mcmc(Mu.fit))
dim(Mu.fit)
cbind(mu.raw, mu.hat, colMeans(Mu.fit))
colMeans(top5.data$top5.mss.case, na.rm = TRUE)
nrow(top5.data$top5.mbs.case)
nrow(top5.data$top5.mbs.ctrl)
round(pos.rates.summary, 3)
bs.tpr.fit = coda.fit[[1]][, grepl("bs_tpr", colnames(coda.fit[[1]]))]
bs.fpr.fit = coda.fit[[1]][, grepl("bs_fpr", colnames(coda.fit[[1]]))]
ss.tpr.fit = coda.fit[[1]][, grepl("ss_tpr", colnames(coda.fit[[1]]))][, 4]
pos.rates = cbind(bs.tpr.fit, bs.fpr.fit, ss.tpr.fit)
pos.rates.summary = rbind(colMeans(pos.rates),
apply(positive.rates, 2,
quantile, c(0.025, 0.975)))
rownames(pos.rates.summary)[1] = "Mean"
round(pos.rates.summary, 3)
pos.rates = cbind(bs.tpr.fit, bs.fpr.fit, ss.tpr.fit)
pos.rates.summary = rbind(colMeans(pos.rates),
apply(pos.rates, 2,
quantile, c(0.025, 0.975)))
rownames(pos.rates.summary)[1] = "Mean"
round(pos.rates.summary, 3)
ss.tpr.fit = coda.fit[[1]][, grepl("ss_tpr", colnames(coda.fit[[1]]))][, 4:5]
pos.rates = cbind(bs.tpr.fit, bs.fpr.fit, ss.tpr.fit)
pos.rates.summary = rbind(colMeans(pos.rates),
apply(pos.rates, 2,
quantile, c(0.025, 0.975)))
rownames(pos.rates.summary)[1] = "Mean"
round(pos.rates.summary, 3)
setwd("~/Documents/JHSPH/Research/S.Zeger/PQ_Models/Code")
source("./RawDataProcess.R")
source("./PerchDataSimulation.R")
source("./ModelFittingJAGS.R")
source("./PostProcessJAGS.R")
source("./PerchPlots.R")
source("../../BAKER/utils.R")
library(ggplot2)
# -----------------------------------------------------------------------------
top5.names = c("PCP", "CMV", "RSV", "PNEU", "SAUR")
saf.raw = read.csv("../WorkSpace/PQ_20170309/rawdata_mobs_SAF_HIV_corrected.csv")
top5.data = GetTop5(saf.raw, top5.names)
# ----------------------------------------------------------------------------
# put in real data
par.default = SetDefaultSimulationParameter(7)
sim.obj = do.call(SimulatePerchData, par.default)
sim.obj$MBS.case = top5.data$top5.mbs.case
sim.obj$MSS.case = top5.data$top5.mss.case
sim.obj$MBS.ctrl = top5.data$top5.mbs.ctrl
sim.obj$X = data.frame(intercept = rep(1, nrow(top5.data$top5.mbs.case)))
# set hyper-parameter
bs.tpr.hyperpar = beta_parms_from_quantiles(c(0.5, 0.9), p = c(0.025, 0.975),
plot = TRUE)
ss.tpr.hyperpar = beta_parms_from_quantiles(c(0.05, 0.2), p = c(0.025, 0.975),
plot = TRUE)
hyper.pars.list = SetDefaultHyperParameters(K = 5)
hyper.pars.list$aa = ss.tpr.hyperpar$a
hyper.pars.list$bb = ss.tpr.hyperpar$b
hyper.pars.list$cc = bs.tpr.hyperpar$a
hyper.pars.list$dd = bs.tpr.hyperpar$b
hyper.pars.list$tau_theta = 0.2
hyper.pars.list$pind_a = 6
hyper.pars.list$pind_b = 2
hyper.pars.list$theta2_mu = -5
# specify model
model.file1 = "./jags/SparseCorr1_BSandSS_NoReg.jags"
jags.result = FitJags(sim.obj,
c("cell_prob", "theta1", "theta2",
"bs_tpr", "ss_tpr", "bs_fpr"),
model.file1, NULL, hyper.pars.list,
15000, 10000, 5, 1)
# collect results
coda.fit = as.mcmc(jags.result)
post.mean = round(summary(coda.fit)[[1]][,1], 3)
Mu.fit = ExtractMu(coda.fit[[1]], sim.obj)
colnames(Mu.fit) = top5.names
plot(as.mcmc(Mu.fit))
summary(as.mcmc(Mu.fit))
Pr.Num.Path.fit = ExtractPrNumPath(coda.fit[[1]], sim.obj)
Pr.Num.Path.fit = cbind(1 - rowSums(Pr.Num.Path.fit), Pr.Num.Path.fit)
round(summary(as.mcmc(Pr.Num.Path.fit))[[1]], 4)
ListEtiology(coda.fit[[1]], sim.obj, top5.names,
reorder = FALSE, num.keep = 16)
ListEtiology(coda.fit[[1]], sim.obj, top5.names,
reorder = FALSE, num.keep = 26)
plot.obj =
PlotByCombination(coda.chains = coda.fit[[1]], sim.obj, hyper.pars.list,
etio.names = top5.names,
contrast = "prior", reorder = FALSE, num.keep = 26,
text.adjust = -1.2)
bs.tpr.fit = coda.fit[[1]][, grepl("bs_tpr", colnames(coda.fit[[1]]))]
bs.fpr.fit = coda.fit[[1]][, grepl("bs_fpr", colnames(coda.fit[[1]]))]
ss.tpr.fit = coda.fit[[1]][, grepl("ss_tpr", colnames(coda.fit[[1]]))][, 4:5]
pos.rates = cbind(bs.tpr.fit, bs.fpr.fit, ss.tpr.fit)
pos.rates.summary = rbind(colMeans(pos.rates),
apply(pos.rates, 2,
quantile, c(0.025, 0.975)))
rownames(pos.rates.summary)[1] = "Mean"
round(pos.rates.summary, 3)
library(rmarkdown)
render("../WorkSpace/PQ_20170309/PQ_model_sync0309.Rmd")
render("../WorkSpace/PQ_20170309/PQ_model_sync0309.Rmd")
render("../WorkSpace/PQ_20170309/PQ_model_sync0309.Rmd")
render("../WorkSpace/PQ_20170309/PQ_model_sync0309.Rmd")
render("../WorkSpace/PQ_20170309/PQ_model_sync0309.Rmd")
render("../WorkSpace/PQ_20170309/PQ_model_sync0309.Rmd")
